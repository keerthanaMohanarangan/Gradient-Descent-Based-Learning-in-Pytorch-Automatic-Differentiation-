{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34004732.0\n",
      "1 30691548.0\n",
      "2 29404502.0\n",
      "3 25964284.0\n",
      "4 19792316.0\n",
      "5 12933832.0\n",
      "6 7622147.5\n",
      "7 4395922.0\n",
      "8 2670303.25\n",
      "9 1775353.75\n",
      "10 1291755.25\n",
      "11 1006767.5\n",
      "12 820687.8125\n",
      "13 687589.4375\n",
      "14 585992.125\n",
      "15 505071.0625\n",
      "16 438755.6875\n",
      "17 383423.78125\n",
      "18 336721.1875\n",
      "19 296988.4375\n",
      "20 262963.875\n",
      "21 233644.890625\n",
      "22 208252.40625\n",
      "23 186170.90625\n",
      "24 166892.609375\n",
      "25 150013.125\n",
      "26 135170.90625\n",
      "27 122073.296875\n",
      "28 110484.3515625\n",
      "29 100206.21875\n",
      "30 91061.8125\n",
      "31 82901.109375\n",
      "32 75612.1484375\n",
      "33 69080.8828125\n",
      "34 63213.9921875\n",
      "35 57933.375\n",
      "36 53170.41015625\n",
      "37 48867.6015625\n",
      "38 44971.9765625\n",
      "39 41441.36328125\n",
      "40 38235.125\n",
      "41 35318.15625\n",
      "42 32661.416015625\n",
      "43 30237.6796875\n",
      "44 28023.26171875\n",
      "45 25998.265625\n",
      "46 24143.814453125\n",
      "47 22445.86328125\n",
      "48 20887.14453125\n",
      "49 19455.17578125\n",
      "50 18137.88671875\n",
      "51 16923.68359375\n",
      "52 15803.478515625\n",
      "53 14769.263671875\n",
      "54 13814.052734375\n",
      "55 12930.16796875\n",
      "56 12111.984375\n",
      "57 11354.416015625\n",
      "58 10651.466796875\n",
      "59 9998.7255859375\n",
      "60 9391.7958984375\n",
      "61 8827.0537109375\n",
      "62 8301.474609375\n",
      "63 7811.4248046875\n",
      "64 7354.5263671875\n",
      "65 6928.04296875\n",
      "66 6529.72314453125\n",
      "67 6157.369140625\n",
      "68 5809.0009765625\n",
      "69 5483.00048828125\n",
      "70 5177.71826171875\n",
      "71 4891.61474609375\n",
      "72 4623.37646484375\n",
      "73 4371.72607421875\n",
      "74 4135.48779296875\n",
      "75 3913.5546875\n",
      "76 3705.00732421875\n",
      "77 3508.88232421875\n",
      "78 3324.3984375\n",
      "79 3150.776611328125\n",
      "80 2987.3408203125\n",
      "81 2833.40234375\n",
      "82 2688.253662109375\n",
      "83 2551.379150390625\n",
      "84 2422.27685546875\n",
      "85 2300.34814453125\n",
      "86 2185.27197265625\n",
      "87 2076.498046875\n",
      "88 1973.704833984375\n",
      "89 1876.54248046875\n",
      "90 1784.71044921875\n",
      "91 1697.955322265625\n",
      "92 1615.8206787109375\n",
      "93 1538.052490234375\n",
      "94 1464.381591796875\n",
      "95 1394.5760498046875\n",
      "96 1328.4208984375\n",
      "97 1265.7054443359375\n",
      "98 1206.197021484375\n",
      "99 1149.73388671875\n",
      "100 1096.15673828125\n",
      "101 1045.285400390625\n",
      "102 996.9559326171875\n",
      "103 951.0698852539062\n",
      "104 907.464111328125\n",
      "105 866.0201416015625\n",
      "106 826.630859375\n",
      "107 789.1740112304688\n",
      "108 753.5406494140625\n",
      "109 719.658935546875\n",
      "110 687.4091796875\n",
      "111 656.7120361328125\n",
      "112 627.5\n",
      "113 599.6835327148438\n",
      "114 573.1861572265625\n",
      "115 547.9409790039062\n",
      "116 523.8779296875\n",
      "117 500.9510192871094\n",
      "118 479.09942626953125\n",
      "119 458.26824951171875\n",
      "120 438.40496826171875\n",
      "121 419.4520263671875\n",
      "122 401.3716735839844\n",
      "123 384.1322326660156\n",
      "124 367.6748046875\n",
      "125 351.96844482421875\n",
      "126 336.9830322265625\n",
      "127 322.67547607421875\n",
      "128 309.0084533691406\n",
      "129 295.9535217285156\n",
      "130 283.4794006347656\n",
      "131 271.5749816894531\n",
      "132 260.1900634765625\n",
      "133 249.30714416503906\n",
      "134 238.9117431640625\n",
      "135 228.97021484375\n",
      "136 219.46778869628906\n",
      "137 210.38552856445312\n",
      "138 201.69442749023438\n",
      "139 193.38406372070312\n",
      "140 185.43527221679688\n",
      "141 177.82737731933594\n",
      "142 170.55520629882812\n",
      "143 163.58905029296875\n",
      "144 156.9263458251953\n",
      "145 150.54840087890625\n",
      "146 144.44085693359375\n",
      "147 138.5960693359375\n",
      "148 132.99986267089844\n",
      "149 127.64229583740234\n",
      "150 122.50938415527344\n",
      "151 117.59207153320312\n",
      "152 112.87989807128906\n",
      "153 108.36752319335938\n",
      "154 104.04237365722656\n",
      "155 99.90071105957031\n",
      "156 95.93017578125\n",
      "157 92.1236343383789\n",
      "158 88.477783203125\n",
      "159 84.98275756835938\n",
      "160 81.6298828125\n",
      "161 78.41677856445312\n",
      "162 75.33391571044922\n",
      "163 72.37945556640625\n",
      "164 69.54469299316406\n",
      "165 66.82537078857422\n",
      "166 64.21849060058594\n",
      "167 61.716224670410156\n",
      "168 59.31632614135742\n",
      "169 57.01365661621094\n",
      "170 54.804176330566406\n",
      "171 52.68429183959961\n",
      "172 50.649658203125\n",
      "173 48.6951904296875\n",
      "174 46.8206787109375\n",
      "175 45.020484924316406\n",
      "176 43.29317092895508\n",
      "177 41.633880615234375\n",
      "178 40.040000915527344\n",
      "179 38.51045227050781\n",
      "180 37.041439056396484\n",
      "181 35.630165100097656\n",
      "182 34.27439880371094\n",
      "183 32.972938537597656\n",
      "184 31.7236328125\n",
      "185 30.5223331451416\n",
      "186 29.368253707885742\n",
      "187 28.258487701416016\n",
      "188 27.19321632385254\n",
      "189 26.16971206665039\n",
      "190 25.185401916503906\n",
      "191 24.239166259765625\n",
      "192 23.330610275268555\n",
      "193 22.456954956054688\n",
      "194 21.61782455444336\n",
      "195 20.810970306396484\n",
      "196 20.035022735595703\n",
      "197 19.290388107299805\n",
      "198 18.572908401489258\n",
      "199 17.88351821899414\n",
      "200 17.220561981201172\n",
      "201 16.582847595214844\n",
      "202 15.969385147094727\n",
      "203 15.380071640014648\n",
      "204 14.813173294067383\n",
      "205 14.267074584960938\n",
      "206 13.742181777954102\n",
      "207 13.236734390258789\n",
      "208 12.750920295715332\n",
      "209 12.283248901367188\n",
      "210 11.833487510681152\n",
      "211 11.400278091430664\n",
      "212 10.983879089355469\n",
      "213 10.582771301269531\n",
      "214 10.196762084960938\n",
      "215 9.825776100158691\n",
      "216 9.468000411987305\n",
      "217 9.124073028564453\n",
      "218 8.792734146118164\n",
      "219 8.47386646270752\n",
      "220 8.166927337646484\n",
      "221 7.871290683746338\n",
      "222 7.586727142333984\n",
      "223 7.313017845153809\n",
      "224 7.049431800842285\n",
      "225 6.795111179351807\n",
      "226 6.550723075866699\n",
      "227 6.315168380737305\n",
      "228 6.088362216949463\n",
      "229 5.870112419128418\n",
      "230 5.660203456878662\n",
      "231 5.457568645477295\n",
      "232 5.262650012969971\n",
      "233 5.074474334716797\n",
      "234 4.893265247344971\n",
      "235 4.718920707702637\n",
      "236 4.550778388977051\n",
      "237 4.388791084289551\n",
      "238 4.232699394226074\n",
      "239 4.082252502441406\n",
      "240 3.937357187271118\n",
      "241 3.7977380752563477\n",
      "242 3.663200855255127\n",
      "243 3.5335192680358887\n",
      "244 3.4083456993103027\n",
      "245 3.2881174087524414\n",
      "246 3.171849012374878\n",
      "247 3.0597827434539795\n",
      "248 2.9518754482269287\n",
      "249 2.847994565963745\n",
      "250 2.7477505207061768\n",
      "251 2.650887966156006\n",
      "252 2.5575642585754395\n",
      "253 2.467893600463867\n",
      "254 2.381314277648926\n",
      "255 2.297651767730713\n",
      "256 2.2172226905822754\n",
      "257 2.139598846435547\n",
      "258 2.0646116733551025\n",
      "259 1.992380976676941\n",
      "260 1.9226711988449097\n",
      "261 1.8556187152862549\n",
      "262 1.7908085584640503\n",
      "263 1.7283456325531006\n",
      "264 1.668094277381897\n",
      "265 1.6098828315734863\n",
      "266 1.5539143085479736\n",
      "267 1.499777913093567\n",
      "268 1.447527527809143\n",
      "269 1.3972764015197754\n",
      "270 1.3488554954528809\n",
      "271 1.3021659851074219\n",
      "272 1.25693678855896\n",
      "273 1.2134333848953247\n",
      "274 1.1713143587112427\n",
      "275 1.1308066844940186\n",
      "276 1.0915744304656982\n",
      "277 1.0538253784179688\n",
      "278 1.017465591430664\n",
      "279 0.9822033643722534\n",
      "280 0.9484001398086548\n",
      "281 0.9156948328018188\n",
      "282 0.8841403126716614\n",
      "283 0.8535884618759155\n",
      "284 0.8242699503898621\n",
      "285 0.7958789467811584\n",
      "286 0.7684960961341858\n",
      "287 0.7420704364776611\n",
      "288 0.7165830135345459\n",
      "289 0.6920015215873718\n",
      "290 0.6681762337684631\n",
      "291 0.6452394723892212\n",
      "292 0.6230713725090027\n",
      "293 0.6017929315567017\n",
      "294 0.5811055302619934\n",
      "295 0.5612108707427979\n",
      "296 0.5420199632644653\n",
      "297 0.5234634280204773\n",
      "298 0.5055884122848511\n",
      "299 0.4883229434490204\n",
      "300 0.4715469479560852\n",
      "301 0.45553961396217346\n",
      "302 0.4400293231010437\n",
      "303 0.425004243850708\n",
      "304 0.4104829728603363\n",
      "305 0.3965071439743042\n",
      "306 0.38293588161468506\n",
      "307 0.3699195981025696\n",
      "308 0.3572509288787842\n",
      "309 0.3451057970523834\n",
      "310 0.33336418867111206\n",
      "311 0.3220401704311371\n",
      "312 0.3111240565776825\n",
      "313 0.3005650043487549\n",
      "314 0.2903980612754822\n",
      "315 0.28052008152008057\n",
      "316 0.270979642868042\n",
      "317 0.2617902457714081\n",
      "318 0.25287777185440063\n",
      "319 0.2443116307258606\n",
      "320 0.23601393401622772\n",
      "321 0.22803552448749542\n",
      "322 0.22030805051326752\n",
      "323 0.21285219490528107\n",
      "324 0.20567940175533295\n",
      "325 0.19868573546409607\n",
      "326 0.19200390577316284\n",
      "327 0.18551650643348694\n",
      "328 0.1792486608028412\n",
      "329 0.17315378785133362\n",
      "330 0.16732804477214813\n",
      "331 0.16170410811901093\n",
      "332 0.15621662139892578\n",
      "333 0.15094545483589172\n",
      "334 0.14586734771728516\n",
      "335 0.14096659421920776\n",
      "336 0.13621996343135834\n",
      "337 0.1316196471452713\n",
      "338 0.1272255778312683\n",
      "339 0.1229247972369194\n",
      "340 0.11881958693265915\n",
      "341 0.11478640139102936\n",
      "342 0.11094321310520172\n",
      "343 0.10722092539072037\n",
      "344 0.10364291816949844\n",
      "345 0.10015497356653214\n",
      "346 0.0968126505613327\n",
      "347 0.09353668987751007\n",
      "348 0.09042808413505554\n",
      "349 0.08737154304981232\n",
      "350 0.0844363123178482\n",
      "351 0.0816144198179245\n",
      "352 0.0788898766040802\n",
      "353 0.07622735947370529\n",
      "354 0.07366076111793518\n",
      "355 0.07122286409139633\n",
      "356 0.06883280724287033\n",
      "357 0.06654255092144012\n",
      "358 0.06433320790529251\n",
      "359 0.062182940542697906\n",
      "360 0.06010763719677925\n",
      "361 0.05809694528579712\n",
      "362 0.05616912618279457\n",
      "363 0.05428807809948921\n",
      "364 0.05248346924781799\n",
      "365 0.05072738602757454\n",
      "366 0.049045585095882416\n",
      "367 0.04739242047071457\n",
      "368 0.045834094285964966\n",
      "369 0.04431010037660599\n",
      "370 0.04283436760306358\n",
      "371 0.0413980670273304\n",
      "372 0.04003404453396797\n",
      "373 0.03868895769119263\n",
      "374 0.037405285984277725\n",
      "375 0.03616226464509964\n",
      "376 0.034959547221660614\n",
      "377 0.033808425068855286\n",
      "378 0.03269089758396149\n",
      "379 0.03159816190600395\n",
      "380 0.030555209144949913\n",
      "381 0.029544711112976074\n",
      "382 0.028571853414177895\n",
      "383 0.027621423825621605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 0.026716526597738266\n",
      "385 0.025844065472483635\n",
      "386 0.024987544864416122\n",
      "387 0.02417268045246601\n",
      "388 0.023380909115076065\n",
      "389 0.02260674722492695\n",
      "390 0.02187269926071167\n",
      "391 0.02115355059504509\n",
      "392 0.02046171948313713\n",
      "393 0.019783109426498413\n",
      "394 0.019144002348184586\n",
      "395 0.018525991588830948\n",
      "396 0.01792045682668686\n",
      "397 0.017332300543785095\n",
      "398 0.016758950427174568\n",
      "399 0.01622135005891323\n",
      "400 0.015686966478824615\n",
      "401 0.015177056193351746\n",
      "402 0.01469375565648079\n",
      "403 0.014210805296897888\n",
      "404 0.013762569054961205\n",
      "405 0.013316741213202477\n",
      "406 0.012884966097772121\n",
      "407 0.012469051405787468\n",
      "408 0.012069438584148884\n",
      "409 0.011676829308271408\n",
      "410 0.011300541460514069\n",
      "411 0.010945262387394905\n",
      "412 0.010592339560389519\n",
      "413 0.01025261078029871\n",
      "414 0.009925587102770805\n",
      "415 0.009607424959540367\n",
      "416 0.009302741847932339\n",
      "417 0.009009167551994324\n",
      "418 0.00871810968965292\n",
      "419 0.008445433340966702\n",
      "420 0.00818035751581192\n",
      "421 0.007920869626104832\n",
      "422 0.007675996050238609\n",
      "423 0.0074311778880655766\n",
      "424 0.007200413849204779\n",
      "425 0.006972213741391897\n",
      "426 0.006757677998393774\n",
      "427 0.006547724828124046\n",
      "428 0.006340207532048225\n",
      "429 0.00614907406270504\n",
      "430 0.005958568304777145\n",
      "431 0.005769014358520508\n",
      "432 0.00559601653367281\n",
      "433 0.005426557268947363\n",
      "434 0.00526209082454443\n",
      "435 0.00509668979793787\n",
      "436 0.004944627173244953\n",
      "437 0.0047917114570736885\n",
      "438 0.004645199980586767\n",
      "439 0.004504051059484482\n",
      "440 0.004368319641798735\n",
      "441 0.00423697754740715\n",
      "442 0.004110453184694052\n",
      "443 0.003989982884377241\n",
      "444 0.003871242981404066\n",
      "445 0.0037566223181784153\n",
      "446 0.003645634278655052\n",
      "447 0.0035383671056479216\n",
      "448 0.0034369933418929577\n",
      "449 0.003335803747177124\n",
      "450 0.00323872291482985\n",
      "451 0.0031420441810041666\n",
      "452 0.003051317296922207\n",
      "453 0.0029637939296662807\n",
      "454 0.002879776293411851\n",
      "455 0.0027988667134195566\n",
      "456 0.0027188837993890047\n",
      "457 0.002642951440066099\n",
      "458 0.0025728982873260975\n",
      "459 0.0024982155300676823\n",
      "460 0.0024278126657009125\n",
      "461 0.0023611767683178186\n",
      "462 0.002296061720699072\n",
      "463 0.002231472870334983\n",
      "464 0.002170983236283064\n",
      "465 0.0021135658025741577\n",
      "466 0.0020546040032058954\n",
      "467 0.0019992897287011147\n",
      "468 0.0019477240275591612\n",
      "469 0.0018961394671350718\n",
      "470 0.0018434020457789302\n",
      "471 0.0017940844409167767\n",
      "472 0.0017448607832193375\n",
      "473 0.0016993015306070447\n",
      "474 0.001656356151215732\n",
      "475 0.001612898544408381\n",
      "476 0.0015723342075943947\n",
      "477 0.0015322016552090645\n",
      "478 0.0014910722384229302\n",
      "479 0.0014531908091157675\n",
      "480 0.0014141437131911516\n",
      "481 0.0013775989646092057\n",
      "482 0.0013426586519926786\n",
      "483 0.0013103927485644817\n",
      "484 0.0012771028559654951\n",
      "485 0.001245357096195221\n",
      "486 0.0012158944737166166\n",
      "487 0.0011867465218529105\n",
      "488 0.0011568294139578938\n",
      "489 0.0011300219921395183\n",
      "490 0.0011026322608813643\n",
      "491 0.0010754212271422148\n",
      "492 0.0010486862156540155\n",
      "493 0.00102279894053936\n",
      "494 0.0009994839783757925\n",
      "495 0.000975433096755296\n",
      "496 0.0009537455043755472\n",
      "497 0.0009301339741796255\n",
      "498 0.0009097959846258163\n",
      "499 0.000889332324732095\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/two_layer_net_autograd.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "device = torch.device('cpu') # Uncomment this to run on CPU\n",
    "\n",
    "# N is batch size;\n",
    "# D_in is input dimension;\n",
    "# H is hidden dimension; \n",
    "#D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# input data : batch of 64 times 1000 features\n",
    "# output data : 100 x 10 continues values (real scalars)\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device, dtype=torch.float)\n",
    "y = torch.randn(N, D_out,device=device,dtype=torch.float)# generate normally distributed data of dim NxD_out, store it on device, requires_grad = False\n",
    "\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "# here the gradient will be computed for the variables that are related to the model learning something new,\n",
    "# i.e. the network weights in this case\n",
    "\n",
    "\n",
    "# WEIGHTS\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)#generate normally distributed data of dim D_in x H, store it on device, requires_grad = True \n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)#generate normally distributed data of dim H x D_out, store it on device, requires_grad = True \n",
    "\n",
    "# initialize loss value to a high number \n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "   y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "   loss = (y_pred - y).pow(2).sum()\n",
    "   print(t, loss.item())\n",
    "   loss.backward()\n",
    "   with torch.no_grad():\n",
    "       w1 -= learning_rate * w1.grad\n",
    "       w2 -= learning_rate * w2.grad\n",
    "       # Manually zero the gradients after updating weights\n",
    "       w1.grad.zero_()\n",
    "       w2.grad.zero_()\n",
    "# initialize arrays errors, w1_array and w2_array to empty lists\n",
    "#errors = # write here\n",
    "#w1_array =  # write here\n",
    "#w2_array =  # write here\n",
    " # set the network learning rate parameter 'learning_rate' to some small number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m37\u001b[0m\n\u001b[1;33m    w1.grad.zero_()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for iteration in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.\n",
    "    # predict the values by multiplying x with weight matrix w1, then apply RELU activation and multiply the result by weight matrix w2\n",
    "    y_pred =F.relu(x.mm(w1).clamp(min=0)).mm(w2) # your code here ; the final prediction is given by matrix multiplying the data \n",
    "    #with the two set of weights, making the intermediate values non-negative (RELU activation function)\n",
    "\n",
    "    # calculate the mean squared error (MSE)\n",
    "    error =  # your code here\n",
    "\n",
    "    \n",
    "    writer.add_scalar(tag=\"Last run\",scalar_value= error, global_step = iteration)\n",
    "    writer.add_histogram(\"error distribution\",error)\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "   \n",
    "    #error.WHAT_FUNCTION_here ?\n",
    "\n",
    "    # Update weights using gradient descent. For this step we just want to mutate\n",
    "    # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "    # to prevent PyTorch from building a computational graph for the updates\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # use w1.grad to update w2 according to the gradient descent formula\n",
    "        # use w2.grad to update w2 according to the gradient descent formula\n",
    "        # also use the learning_rate you set before!\n",
    "         w1 -= learning_rate * w1.grad\n",
    "         w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        \n",
    "    if iteration % 50 == 0:\n",
    "        print(\"Iteration: %d - Error: %.4f\" % (iteration, error))\n",
    "        w1_array.append(w1.cpu().detach().numpy())\n",
    "        w2_array.append(w2.cpu().detach().numpy())\n",
    "        errors.append(error.cpu().detach().numpy())\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "    if loss_value < 1e-6:\n",
    "        print(\"Stopping gradient descent, algorithm converged, MSE loss is smaller than 1E-6\")\n",
    "        break\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation between backpropagation used in neural networks and Automatic Differentiation ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is a supervisied learning algorithm used to train neural networks to find the least value of the error function. It uses the gradient descent method  and the weights with the least error function are considered to be the answer to the learning problem. It calculates the weights update in order to improve the network until it can perform its’ task. Backpropagation requires the derivative of the activation functions to be known at design time.\n",
    "Automatic differentiation is a method that can automatically and analytically provide the derivatives to the training algorithm.\n",
    "It provides the derivatives for the backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
